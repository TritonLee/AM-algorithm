{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled14.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNidZarxpx8pFXQMRrHeaod",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TritonLee/AM-algorithm/blob/main/Stochastic_gradient_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGqZ7qM5qCbX"
      },
      "source": [
        "def gradient_descent(gradient, start, learn_rate, n_iter):\n",
        "     vector = start\n",
        "     for _ in range(n_iter):\n",
        "         diff = -learn_rate * gradient(vector)\n",
        "         vector += diff\n",
        "     return vector"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oZ2pVgBq8Cr",
        "outputId": "4f5bc471-e5d9-4c20-dafe-cf6a156b3aa7"
      },
      "source": [
        "gradient_descent(gradient=lambda v: 2 * v, start=0.1, learn_rate=0.005, n_iter =800000)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.4e-322"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSw7v_qewHYE"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "def gradient_descent(gradient,x,y,start,learn_rate=0.1,n_iter=50,tolerance=1e-06,dtype=\"float64\"):\n",
        "    # Checking if the gradient is callable\n",
        "    if not callable(gradient):\n",
        "        raise TypeError(\"'gradient' must be callable\")\n",
        "\n",
        "    # Setting up the data type for NumPy arrays\n",
        "    dtype_ = np.dtype(dtype)\n",
        "\n",
        "    # Converting x and y to NumPy arrays\n",
        "    x, y = np.array(x, dtype=dtype_), np.array(y, dtype=dtype_)\n",
        "    if x.shape[0] != y.shape[0]:\n",
        "        raise ValueError(\"'x' and 'y' lengths do not match\")\n",
        "\n",
        "    # Initializing the values of the variables\n",
        "    vector = np.array(start, dtype=dtype_)\n",
        "      # Setting up and checking the learning rate\n",
        "    learn_rate = np.array(learn_rate, dtype=dtype_)\n",
        "    if np.any(learn_rate <= 0):\n",
        "        raise ValueError(\"'learn_rate' must be greater than zero\")\n",
        "\n",
        "    # Setting up and checking the maximal number of iterations\n",
        "    n_iter = int(n_iter)\n",
        "    if n_iter <= 0:\n",
        "        raise ValueError(\"'n_iter' must be greater than zero\")\n",
        "\n",
        "  # Setting up and checking the tolerance\n",
        "    tolerance = np.array(tolerance, dtype=dtype_)\n",
        "    if np.any(tolerance <= 0):\n",
        "        raise ValueError(\"'tolerance' must be greater than zero\")\n",
        "      # Performing the gradient descent loop\n",
        "    for _ in range(n_iter):\n",
        "      # Recalculating the difference\n",
        "        diff = -learn_rate * np.array(gradient(x, y, vector), dtype_)\n",
        "\n",
        "      # Checking if the absolute difference is small enough\n",
        "        if np.all(np.abs(diff) <= tolerance):\n",
        "            break\n",
        "\n",
        "      # Updating the values of the variables\n",
        "        vector += diff\n",
        "\n",
        "    return vector if vector.shape else vector.item()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WcPIihXykmR"
      },
      "source": [
        "def ssr_gradient(x, y, b):\n",
        "    res = b[0] + b[1] * x - y\n",
        "    return res.mean(), (res * x).mean()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nflS2sj7yka-"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l84CTGrwyFhl"
      },
      "source": [
        " x = np.array([5, 15, 25, 35, 45, 55])\n",
        " y = np.array([5, 20, 14, 32, 22, 38])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22Vtoez8yRHB",
        "outputId": "2273c2c1-7679-47d4-c43b-7949d8656f20"
      },
      "source": [
        "start_time=time.time()\n",
        "gradient_descent(ssr_gradient, x, y, start=[0.5, 0.5], learn_rate=0.0008,n_iter=100)\n",
        "total_time=time.time()-start_time\n",
        "print('time_cost:%s'%total_time)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time_cost:0.0016598701477050781\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}